{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"StpwZzOYxFRo","executionInfo":{"status":"ok","timestamp":1702325360118,"user_tz":-330,"elapsed":7335,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[],"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","import pickle\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","assert tf.__version__.startswith('2')\n","\n","import PIL\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import Input, Model\n","from tensorflow.keras.layers import LeakyReLU, BatchNormalization, ReLU, Activation\n","from tensorflow.keras.layers import UpSampling2D, Conv2D, Concatenate, Dense, concatenate\n","from tensorflow.keras.layers import Flatten, Lambda, Reshape, ZeroPadding2D, add\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{"id":"8Rb4IqOBxN1C"},"source":["\n","############################################################\n","# Conditioning Augmentation Network\n","############################################################\n","\n","\n","Computer doesn’t understand words, but it can represent the words in terms of something it does “understand”. That’s the “text embedding”, and it’s used as the c\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"SO_uUHnyxNOU","executionInfo":{"status":"ok","timestamp":1702325362126,"user_tz":-330,"elapsed":468,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[],"source":["# conditioned by the text.\n","def conditioning_augmentation(x):\n","\t\"\"\"The mean_logsigma passed as argument is converted into the text conditioning variable.\n","\n","\tArgs:\n","\t\tx: The output of the text embedding passed through a FC layer with LeakyReLU non-linearity.\n","\n","\tReturns:\n","\t \tc: The text conditioning variable after computation.\n","\t\"\"\"\n","\tmean = x[:, :128]\n","\tlog_sigma = x[:, 128:]\n","\n","\tstddev = tf.math.exp(log_sigma)\n","\tepsilon = K.random_normal(shape=K.constant((mean.shape[1], ), dtype='int32'))\n","\tc = mean + stddev * epsilon\n","\treturn c\n","\n","def build_ca_network():\n","\t\"\"\"Builds the conditioning augmentation network.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(1024,)) #size of the vocabulary in the text data\n","\tmls = Dense(256)(input_layer1)\n","\tmls = LeakyReLU(alpha=0.2)(mls)\n","\tca = Lambda(conditioning_augmentation)(mls)\n","\treturn Model(inputs=[input_layer1], outputs=[ca])\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KY79lRr0pqnq","executionInfo":{"status":"ok","timestamp":1702325408004,"user_tz":-330,"elapsed":43215,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}},"outputId":"b9dc4ab2-fd8e-469a-8875-9b36a988ae78"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"-Ad1kreZoKNd"},"source":["############################################################\n","# Stage 1 Generator Network\n","############################################################\n","\n","1. The generator is fed with the text captions in the form of Embedding vectors which will be used to condition its generation of features.\n","2. A vector with random noise.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8BaEBiaCxFUd","executionInfo":{"status":"ok","timestamp":1702325413584,"user_tz":-330,"elapsed":453,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[],"source":["def UpSamplingBlock(x, num_kernels):\n","\t\"\"\"An Upsample block with Upsampling2D, Conv2D, BatchNormalization and a ReLU activation.\n","\n","\tArgs:\n","\t\tx: The preceding layer as input.\n","\t\tnum_kernels: Number of kernels for the Conv2D layer.\n","\n","\tReturns:\n","\t\tx: The final activation layer after the Upsampling block.\n","\t\"\"\"\n","\tx = UpSampling2D(size=(2,2))(x)\n","\tx = Conv2D(num_kernels, kernel_size=(3,3), padding='same', strides=1, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x) #prevent from mode collapse\n","\tx = ReLU()(x)\n","\treturn x\n","\n","\n","def build_stage1_generator():\n","\n","\tinput_layer1 = Input(shape=(1024,))\n","\tca = Dense(256)(input_layer1)\n","\tca = LeakyReLU(alpha=0.2)(ca)\n","\n","\t# Obtain the conditioned text\n","\tc = Lambda(conditioning_augmentation)(ca)\n","\n","\tinput_layer2 = Input(shape=(100,))\n","\tconcat = Concatenate(axis=1)([c, input_layer2])\n","\n","\tx = Dense(16384, use_bias=False)(concat)\n","\tx = ReLU()(x)\n","\tx = Reshape((4, 4, 1024), input_shape=(16384,))(x)\n","\n","\tx = UpSamplingBlock(x, 512)\n","\tx = UpSamplingBlock(x, 256)\n","\tx = UpSamplingBlock(x, 128)\n","\tx = UpSamplingBlock(x, 64)   # upsampled our image to 64*64*3\n","\n","\tx = Conv2D(3, kernel_size=3, padding='same', strides=1, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = Activation('tanh')(x)\n","\n","\tstage1_gen = Model(inputs=[input_layer1, input_layer2], outputs=[x, ca])\n","\treturn stage1_gen\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SdmCH2K4oKNe","executionInfo":{"status":"ok","timestamp":1702325425110,"user_tz":-330,"elapsed":5157,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}},"outputId":"8a79fb0a-e7a9-462a-b187-8a0c7386726f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 1024)]               0         []                            \n","                                                                                                  \n"," dense (Dense)               (None, 256)                  262400    ['input_1[0][0]']             \n","                                                                                                  \n"," leaky_re_lu (LeakyReLU)     (None, 256)                  0         ['dense[0][0]']               \n","                                                                                                  \n"," lambda (Lambda)             (None, 128)                  0         ['leaky_re_lu[0][0]']         \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 100)]                0         []                            \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 228)                  0         ['lambda[0][0]',              \n","                                                                     'input_2[0][0]']             \n","                                                                                                  \n"," dense_1 (Dense)             (None, 16384)                3735552   ['concatenate[0][0]']         \n","                                                                                                  \n"," re_lu (ReLU)                (None, 16384)                0         ['dense_1[0][0]']             \n","                                                                                                  \n"," reshape (Reshape)           (None, 4, 4, 1024)           0         ['re_lu[0][0]']               \n","                                                                                                  \n"," up_sampling2d (UpSampling2  (None, 8, 8, 1024)           0         ['reshape[0][0]']             \n"," D)                                                                                               \n","                                                                                                  \n"," conv2d (Conv2D)             (None, 8, 8, 512)            4718592   ['up_sampling2d[0][0]']       \n","                                                                                                  \n"," batch_normalization (Batch  (None, 8, 8, 512)            2048      ['conv2d[0][0]']              \n"," Normalization)                                                                                   \n","                                                                                                  \n"," re_lu_1 (ReLU)              (None, 8, 8, 512)            0         ['batch_normalization[0][0]'] \n","                                                                                                  \n"," up_sampling2d_1 (UpSamplin  (None, 16, 16, 512)          0         ['re_lu_1[0][0]']             \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_1 (Conv2D)           (None, 16, 16, 256)          1179648   ['up_sampling2d_1[0][0]']     \n","                                                                                                  \n"," batch_normalization_1 (Bat  (None, 16, 16, 256)          1024      ['conv2d_1[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," re_lu_2 (ReLU)              (None, 16, 16, 256)          0         ['batch_normalization_1[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," up_sampling2d_2 (UpSamplin  (None, 32, 32, 256)          0         ['re_lu_2[0][0]']             \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_2 (Conv2D)           (None, 32, 32, 128)          294912    ['up_sampling2d_2[0][0]']     \n","                                                                                                  \n"," batch_normalization_2 (Bat  (None, 32, 32, 128)          512       ['conv2d_2[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," re_lu_3 (ReLU)              (None, 32, 32, 128)          0         ['batch_normalization_2[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," up_sampling2d_3 (UpSamplin  (None, 64, 64, 128)          0         ['re_lu_3[0][0]']             \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_3 (Conv2D)           (None, 64, 64, 64)           73728     ['up_sampling2d_3[0][0]']     \n","                                                                                                  \n"," batch_normalization_3 (Bat  (None, 64, 64, 64)           256       ['conv2d_3[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," re_lu_4 (ReLU)              (None, 64, 64, 64)           0         ['batch_normalization_3[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_4 (Conv2D)           (None, 64, 64, 3)            1728      ['re_lu_4[0][0]']             \n","                                                                                                  \n"," activation (Activation)     (None, 64, 64, 3)            0         ['conv2d_4[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 10270400 (39.18 MB)\n","Trainable params: 10268480 (39.17 MB)\n","Non-trainable params: 1920 (7.50 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["generator = build_stage1_generator()\n","generator.summary()"]},{"cell_type":"markdown","metadata":{"id":"bc7hLipAxcXb"},"source":["\n","############################################################\n","# Stage 1 Discriminator Network\n","############################################################\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"JwHBP8sCxFXg","executionInfo":{"status":"ok","timestamp":1702325432200,"user_tz":-330,"elapsed":426,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[],"source":["def ConvBlock(x, num_kernels, kernel_size=(4,4), strides=2, activation=True):\n","\t\"\"\"A ConvBlock with a Conv2D, BatchNormalization and LeakyReLU activation.\n","\n","\tArgs:\n","\t\tx: The preceding layer as input.\n","\t\tnum_kernels: Number of kernels for the Conv2D layer.\n","\n","\tReturns:\n","\t\tx: The final activation layer after the ConvBlock block.\n","\t\"\"\"\n","\tx = Conv2D(num_kernels, kernel_size=kernel_size, padding='same', strides=strides, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\n","\tif activation:\n","\t\tx = LeakyReLU(alpha=0.2)(x)\n","\treturn x\n","\n","\n","def build_embedding_compressor():\n","    \"\"\"Build embedding compressor model\n","    \"\"\"\n","    input_layer1 = Input(shape=(1024,))\n","    x = Dense(128)(input_layer1)\n","    x = ReLU()(x)\n","\n","    model = Model(inputs=[input_layer1], outputs=[x])\n","    return model\n","\n","# the discriminator is fed with two inputs, the feature from Generator and the text embedding\n","def build_stage1_discriminator():\n","\t\"\"\"Builds the Stage 1 Discriminator that uses the 64x64 resolution images from the generator\n","\tand the compressed and spatially replicated embedding.\n","\n","\tReturns:\n","\t\tStage 1 Discriminator Model for StackGAN.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(64, 64, 3))\n","\n","\tx = Conv2D(64, kernel_size=(4,4), strides=2, padding='same', use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(input_layer1)\n","\tx = LeakyReLU(alpha=0.2)(x)\n","\n","\tx = ConvBlock(x, 128)\n","\tx = ConvBlock(x, 256)\n","\tx = ConvBlock(x, 512)\n","\n","\t# Obtain the compressed and spatially replicated text embedding\n","\tinput_layer2 = Input(shape=(4, 4, 128)) #2nd input to discriminator, text embedding\n","\tconcat = concatenate([x, input_layer2])\n","\n","\tx1 = Conv2D(512, kernel_size=(1,1), padding='same', strides=1, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(concat)\n","\tx1 = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx1 = LeakyReLU(alpha=0.2)(x)\n","\n","\t# Flatten and add a FC layer to predict.\n","\tx1 = Flatten()(x1)\n","\tx1 = Dense(1)(x1)\n","\tx1 = Activation('sigmoid')(x1)\n","\n","\tstage1_dis = Model(inputs=[input_layer1, input_layer2], outputs=[x1])\n","\treturn stage1_dis\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_3TqeproKNh","executionInfo":{"status":"ok","timestamp":1702325436447,"user_tz":-330,"elapsed":36,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}},"outputId":"29eb4a06-ab2e-4cf7-9259-24705c01264a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_3 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n","                                                                                                  \n"," conv2d_5 (Conv2D)           (None, 32, 32, 64)           3072      ['input_3[0][0]']             \n","                                                                                                  \n"," leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 64)           0         ['conv2d_5[0][0]']            \n","                                                                                                  \n"," conv2d_6 (Conv2D)           (None, 16, 16, 128)          131072    ['leaky_re_lu_1[0][0]']       \n","                                                                                                  \n"," batch_normalization_4 (Bat  (None, 16, 16, 128)          512       ['conv2d_6[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)          0         ['batch_normalization_4[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_7 (Conv2D)           (None, 8, 8, 256)            524288    ['leaky_re_lu_2[0][0]']       \n","                                                                                                  \n"," batch_normalization_5 (Bat  (None, 8, 8, 256)            1024      ['conv2d_7[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," leaky_re_lu_3 (LeakyReLU)   (None, 8, 8, 256)            0         ['batch_normalization_5[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," conv2d_8 (Conv2D)           (None, 4, 4, 512)            2097152   ['leaky_re_lu_3[0][0]']       \n","                                                                                                  \n"," batch_normalization_6 (Bat  (None, 4, 4, 512)            2048      ['conv2d_8[0][0]']            \n"," chNormalization)                                                                                 \n","                                                                                                  \n"," leaky_re_lu_4 (LeakyReLU)   (None, 4, 4, 512)            0         ['batch_normalization_6[0][0]'\n","                                                                    ]                             \n","                                                                                                  \n"," leaky_re_lu_5 (LeakyReLU)   (None, 4, 4, 512)            0         ['leaky_re_lu_4[0][0]']       \n","                                                                                                  \n"," flatten (Flatten)           (None, 8192)                 0         ['leaky_re_lu_5[0][0]']       \n","                                                                                                  \n"," dense_2 (Dense)             (None, 1)                    8193      ['flatten[0][0]']             \n","                                                                                                  \n"," input_4 (InputLayer)        [(None, 4, 4, 128)]          0         []                            \n","                                                                                                  \n"," activation_1 (Activation)   (None, 1)                    0         ['dense_2[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 2767361 (10.56 MB)\n","Trainable params: 2765569 (10.55 MB)\n","Non-trainable params: 1792 (7.00 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["discriminator = build_stage1_discriminator()\n","discriminator.summary()"]},{"cell_type":"markdown","metadata":{"id":"SXNLnmkFxjI0"},"source":["\n","############################################################\n","# Stage 1 Adversarial Model  (Building a GAN)\n","############################################################\n","\n","Generator and discriminator are stacked together. Output of the former is the input of the latter."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"8rjVyqYhxFag","executionInfo":{"status":"ok","timestamp":1702325448514,"user_tz":-330,"elapsed":423,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[],"source":["# Building GAN with Generator and Discriminator\n","\n","def build_adversarial(generator_model, discriminator_model):\n","\t\"\"\"Stage 1 Adversarial model.\n","\n","\tArgs:\n","\t\tgenerator_model: Stage 1 Generator Model\n","\t\tdiscriminator_model: Stage 1 Discriminator Model\n","\n","\tReturns:\n","\t\tAdversarial Model.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(1024,))\n","\tinput_layer2 = Input(shape=(100,))\n","\tinput_layer3 = Input(shape=(4, 4, 128))\n","\n","\tx, ca = generator_model([input_layer1, input_layer2]) #text,noise\n","\n","\tdiscriminator_model.trainable = False\n","\n","\tprobabilities = discriminator_model([x, input_layer3])\n","\tadversarial_model = Model(inputs=[input_layer1, input_layer2, input_layer3], outputs=[probabilities, ca])\n","\treturn adversarial_model\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"lhD7xJWnoKNi","outputId":"d4f4bb68-7bb9-4736-aa1f-e10d97a1d20d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702325455160,"user_tz":-330,"elapsed":7,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_5 (InputLayer)        [(None, 1024)]               0         []                            \n","                                                                                                  \n"," input_6 (InputLayer)        [(None, 100)]                0         []                            \n","                                                                                                  \n"," model (Functional)          [(None, 64, 64, 3),          1027040   ['input_5[0][0]',             \n","                              (None, 256)]                0          'input_6[0][0]']             \n","                                                                                                  \n"," input_7 (InputLayer)        [(None, 4, 4, 128)]          0         []                            \n","                                                                                                  \n"," model_1 (Functional)        (None, 1)                    2767361   ['model[0][0]',               \n","                                                                     'input_7[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 13037761 (49.74 MB)\n","Trainable params: 10268480 (39.17 MB)\n","Non-trainable params: 2769281 (10.56 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["ganstage1 = build_adversarial(generator, discriminator)\n","ganstage1.summary()"]},{"cell_type":"markdown","metadata":{"id":"g4Dy_RuO42Am"},"source":["############################################################\n","# Train Utilities\n","############################################################\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"tuXx7HDj41XW","executionInfo":{"status":"ok","timestamp":1702325460348,"user_tz":-330,"elapsed":3,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[],"source":["\n","def checkpoint_prefix():\n","\tcheckpoint_dir = './training_checkpoints'\n","\tcheckpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n","\n","\treturn checkpoint_prefix\n","\n","def adversarial_loss(y_true, y_pred):\n","\tmean = y_pred[:, :128]\n","\tls = y_pred[:, 128:]\n","\tloss = -ls + 0.5 * (-1 + tf.math.exp(2.0 * ls) + tf.math.square(mean))\n","\tloss = K.mean(loss)\n","\treturn loss\n","\n","def normalize(input_image, real_image):\n","\tinput_image = (input_image / 127.5) - 1\n","\treal_image = (real_image / 127.5) - 1\n","\n","\treturn input_image, real_image\n","\n","def load_class_ids_filenames(class_id_path, filename_path):\n","\twith open(class_id_path, 'rb') as file:\n","\t\tclass_id = pickle.load(file, encoding='latin1')\n","\n","\twith open(filename_path, 'rb') as file:\n","\t\tfilename = pickle.load(file, encoding='latin1')\n","\n","\treturn class_id, filename\n","\n","def load_text_embeddings(text_embeddings):\n","\twith open(text_embeddings, 'rb') as file:\n","\t\tembeds = pickle.load(file, encoding='latin1')\n","\t\tembeds = np.array(embeds)\n","\n","\treturn embeds\n","\n","def load_bbox(data_path):\n","\tbbox_path = data_path + '/bounding_boxes.txt'\n","\timage_path = data_path + '/images.txt'\n","\tbbox_df = pd.read_csv(bbox_path, delim_whitespace=True, header=None).astype(int)\n","\tfilename_df = pd.read_csv(image_path, delim_whitespace=True, header=None)\n","\n","\tfilenames = filename_df[1].tolist()\n","\tbbox_dict = {i[:-4]:[] for i in filenames[:2]}\n","\n","\tfor i in range(0, len(filenames)):\n","\t\tbbox = bbox_df.iloc[i][1:].tolist()\n","\t\tdict_key = filenames[i][:-4]\n","\t\tbbox_dict[dict_key] = bbox\n","\n","\treturn bbox_dict\n","\n","def load_images(image_path, bounding_box, size):\n","\t\"\"\"Crops the image to the bounding box and then resizes it.\n","\t\"\"\"\n","\timage = Image.open(image_path).convert('RGB')\n","\tw, h = image.size\n","\tif bounding_box is not None:\n","\t\tr = int(np.maximum(bounding_box[2], bounding_box[3]) * 0.75)\n","\t\tc_x = int((bounding_box[0] + bounding_box[2]) / 2)\n","\t\tc_y = int((bounding_box[1] + bounding_box[3]) / 2)\n","\t\ty1 = np.maximum(0, c_y - r)\n","\t\ty2 = np.minimum(h, c_y + r)\n","\t\tx1 = np.maximum(0, c_x - r)\n","\t\tx2 = np.minimum(w, c_x + r)\n","\t\timage = image.crop([x1, y1, x2, y2])\n","\n","\timage = image.resize(size, PIL.Image.BILINEAR)\n","\treturn image\n","\n","def load_data(filename_path, class_id_path, dataset_path, embeddings_path, size):\n","\t\"\"\"Loads the Dataset.\n","\t\"\"\"\n","\tdata_dir = \"/content/drive/MyDrive/birds\"\n","\ttrain_dir = data_dir + \"/train\"\n","\ttest_dir = data_dir + \"/test\"\n","\tembeddings_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","\tembeddings_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n","\tfilename_path_train = train_dir + \"/filenames.pickle\"\n","\tfilename_path_test = test_dir + \"/filenames.pickle\"\n","\tclass_id_path_train = train_dir + \"/class_info.pickle\"\n","\tclass_id_path_test = test_dir + \"/class_info.pickle\"\n","\tdataset_path = \"/content/drive/MyDrive/CUB_200_2011\"\n","\tclass_id, filenames = load_class_ids_filenames(class_id_path, filename_path)\n","\tembeddings = load_text_embeddings(embeddings_path)\n","\tbbox_dict = load_bbox(dataset_path)\n","\n","\tx, y, embeds = [], [], []\n","\n","\tfor i, filename in enumerate(filenames):\n","\t\tbbox = bbox_dict[filename]\n","\n","\t\ttry:\n","\t\t\timage_path = f'{dataset_path}/images/{filename}.jpg'\n","\t\t\timage = load_images(image_path, bbox, size)\n","\t\t\te = embeddings[i, :, :]\n","\t\t\tembed_index = np.random.randint(0, e.shape[0] - 1)\n","\t\t\tembed = e[embed_index, :]\n","\n","\t\t\tx.append(np.array(image))\n","\t\t\ty.append(class_id[i])\n","\t\t\tembeds.append(embed)\n","\n","\t\texcept Exception as e:\n","\t\t\tprint(f'{e}')\n","\n","\tx = np.array(x)\n","\ty = np.array(y)\n","\tembeds = np.array(embeds)\n","\n","\treturn x, y, embeds\n","\n","def save_image(file, save_path):\n","\t\"\"\"Saves the image at the specified file path.\n","\t\"\"\"\n","\timage = plt.figure()\n","\tax = image.add_subplot(1,1,1)\n","\tax.imshow(file)\n","\tax.axis(\"off\")\n","\tplt.savefig(save_path)\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pjcH1V8ax9A1","executionInfo":{"status":"ok","timestamp":1702325465616,"user_tz":-330,"elapsed":670,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[],"source":["\n","############################################################\n","# StackGAN class\n","############################################################\n","\n","class StackGanStage1(object):\n","\t\tdata_dir = \"/content/drive/MyDrive/birds\"\n","\t\ttrain_dir = \"/content/drive/MyDrive/birds/train\"\n","\t\ttest_dir = \"/content/drive/MyDrive/birds/test\"\n","\t\tembeddings_path_train = \"/content/drive/MyDrive/birds/train/char-CNN-RNN-embeddings.pickle\"\n","\t\tembeddings_path_test = \"/content/drive/MyDrive/birds/test/char-CNN-RNN-embeddings.pickle\"\n","\t\tfilename_path_train = \"/content/drive/MyDrive/birds/train/filenames.pickle\"\n","\t\tfilename_path_test = \"/content/drive/MyDrive/birds/test/filenames.pickle\"\n","\t\tclass_id_path_train = \"/content/drive/MyDrive/birds/train/class_info.pickle\"\n","\t\tclass_id_path_test = \"/content/drive/MyDrive/birds/test/class_info.pickle\"\n","\t\tdataset_path = \"/content/drive/MyDrive/CUB_200_2011\"\n","\n","\t\tdef __init__(self, epochs=500, z_dim=100, batch_size=64, enable_function=True, stage1_generator_lr=0.0002, stage1_discriminator_lr=0.0002):\n","\t\t\t\tself.epochs = epochs\n","\t\t\t\tself.z_dim = z_dim\n","\t\t\t\tself.enable_function = enable_function\n","\t\t\t\tself.stage1_generator_lr = stage1_generator_lr\n","\t\t\t\tself.stage1_discriminator_lr = stage1_discriminator_lr\n","\t\t\t\tself.image_size = 64\n","\t\t\t\tself.conditioning_dim = 128\n","\t\t\t\tself.batch_size = batch_size\n","\n","\t\t\t\tself.stage1_generator_optimizer = Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n","\t\t\t\tself.stage1_discriminator_optimizer = Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","\n","\t\t\t\tself.stage1_generator = build_stage1_generator()\n","\t\t\t\tself.stage1_generator.compile(loss='mse', optimizer=self.stage1_generator_optimizer)\n","\n","\t\t\t\tself.stage1_discriminator = build_stage1_discriminator()\n","\t\t\t\tself.stage1_discriminator.compile(loss='binary_crossentropy', optimizer=self.stage1_discriminator_optimizer)\n","\n","\t\t\t\tself.ca_network = build_ca_network()\n","\t\t\t\tself.ca_network.compile(loss='binary_crossentropy', optimizer='Adam')\n","\n","\t\t\t\tself.embedding_compressor = build_embedding_compressor()\n","\t\t\t\tself.embedding_compressor.compile(loss='binary_crossentropy', optimizer='Adam')\n","\n","\t\t\t\tself.stage1_adversarial = build_adversarial(self.stage1_generator, self.stage1_discriminator)\n","\t\t\t\tself.stage1_adversarial.compile(loss=['binary_crossentropy', adversarial_loss], loss_weights=[1, 2.0], optimizer=self.stage1_generator_optimizer)\n","\n","\t\t\t\tself.checkpoint1 = tf.train.Checkpoint(\n","\t\t\t\t\t\t\tgenerator_optimizer=self.stage1_generator_optimizer,\n","\t\t\t\t\t\t\tdiscriminator_optimizer=self.stage1_discriminator_optimizer,\n","\t\t\t\t\t\t\tgenerator=self.stage1_generator,\n","\t\t\t\t\t\t\tdiscriminator=self.stage1_discriminator)\n","\n","\t\tdef visualize_stage1(self):\n","\t\t\t\ttb = TensorBoard(log_dir=\"/content/drive/MyDrive/logs\".format(time.time()))\n","\t\t\t\ttb.set_model(self.stage1_generator)\n","\t\t\t\ttb.set_model(self.stage1_discriminator)\n","\t\t\t\ttb.set_model(self.ca_network)\n","\t\t\t\ttb.set_model(self.embedding_compressor)\n","\n","\t\tdef train_stage1(self):\n","\t\t\t\tx_train, y_train, train_embeds = load_data(filename_path=\"/content/drive/MyDrive/birds/train/filenames.pickle\", class_id_path=\"/content/drive/MyDrive/birds/train/class_info.pickle\", dataset_path=\"/content/drive/MyDrive/CUB_200_2011\", embeddings_path=\"/content/drive/MyDrive/birds/train/char-CNN-RNN-embeddings.pickle\", size=(64, 64))\n","\t\t\t\tx_test, y_test, test_embeds = load_data(filename_path=\"/content/drive/MyDrive/birds/test/filenames.pickle\", class_id_path=\"/content/drive/MyDrive/birds/test/class_info.pickle\", dataset_path=\"/content/drive/MyDrive/CUB_200_2011\", embeddings_path=\"/content/drive/MyDrive/birds/test/char-CNN-RNN-embeddings.pickle\", size=(64, 64))\n","\n","\t\t\t\treal = np.ones((self.batch_size, 1), dtype='float') * 0.9\n","\t\t\t\tfake = np.zeros((self.batch_size, 1), dtype='float') * 0.1\n","\n","\t\t\t\tfor epoch in range(self.epochs):\n","\t\t\t\t\tprint(f'Epoch: {epoch}')\n","\n","\t\t\t\t\tgen_loss = []\n","\t\t\t\t\tdis_loss = []\n","\n","\t\t\t\t\tnum_batches = int(x_train.shape[0] / self.batch_size)\n","\n","\t\t\t\t\tfor i in range(num_batches):\n","\n","\t\t\t\t\t\tlatent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n","\t\t\t\t\t\tembedding_text = train_embeds[i * self.batch_size:(i + 1) * self.batch_size]\n","\t\t\t\t\t\tcompressed_embedding = self.embedding_compressor.predict_on_batch(embedding_text)\n","\t\t\t\t\t\tcompressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, 128))\n","\t\t\t\t\t\tcompressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","\t\t\t\t\t\timage_batch = x_train[i * self.batch_size:(i+1) * self.batch_size]\n","\t\t\t\t\t\timage_batch = (image_batch - 127.5) / 127.5\n","\n","\t\t\t\t\t\tgen_images, _ = self.stage1_generator.predict([embedding_text, latent_space])\n","\n","\t\t\t\t\t\tdiscriminator_loss = self.stage1_discriminator.train_on_batch([image_batch, compressed_embedding],\n","\t\t\t\t\t\t\tnp.reshape(real, (self.batch_size, 1)))\n","\n","\t\t\t\t\t\tdiscriminator_loss_gen = self.stage1_discriminator.train_on_batch([gen_images, compressed_embedding],\n","\t\t\t\t\t\t\tnp.reshape(fake, (self.batch_size, 1)))\n","\n","\t\t\t\t\t\tdiscriminator_loss_wrong = self.stage1_discriminator.train_on_batch([gen_images[: self.batch_size-1], compressed_embedding[1:]],\n","\t\t\t\t\t\t\tnp.reshape(fake[1:], (self.batch_size-1, 1)))\n","\n","\t\t\t\t\t\t# Discriminator loss\n","\t\t\t\t\t\td_loss = 0.5 * np.add(discriminator_loss, 0.5 * np.add(discriminator_loss_gen, discriminator_loss_wrong))\n","\t\t\t\t\t\tdis_loss.append(d_loss)\n","\n","\t\t\t\t\t\tprint(f'Discriminator Loss: {d_loss}')\n","\n","\t\t\t\t\t\t# Generator loss\n","\t\t\t\t\t\tg_loss = self.stage1_adversarial.train_on_batch([embedding_text, latent_space, compressed_embedding],[K.ones((self.batch_size, 1)) * 0.9, K.ones((self.batch_size, 256)) * 0.9])\n","\n","\t\t\t\t\t\tprint(f'Generator Loss: {g_loss}')\n","\t\t\t\t\t\tgen_loss.append(g_loss)\n","\n","\t\t\t\t\t\tif epoch % 5 == 0:\n","\t\t\t\t\t\t\t\tlatent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n","\t\t\t\t\t\t\t\tembedding_batch = test_embeds[0 : self.batch_size]\n","\t\t\t\t\t\t\t\tgen_images, _ = self.stage1_generator.predict_on_batch([embedding_batch, latent_space])\n","\n","\t\t\t\t\t\t\t\tfor i, image in enumerate(gen_images[:10]):\n","\t\t\t\t\t\t\t\t\t\tsave_image(image, f'/content/drive/MyDrive/birds/test/gen_1_{epoch}_{i}')\n","\n","\t\t\t\t\t\tif epoch % 25 == 0:\n","\t\t\t\t\t\t\tself.stage1_generator.save_weights('/content/drive/MyDrive/weights/stage1_gen.h5')\n","\t\t\t\t\t\t\tself.stage1_discriminator.save_weights(\"/content/drive/MyDrive/weights/stage1_disc.h5\")\n","\t\t\t\t\t\t\tself.ca_network.save_weights('/content/drive/MyDrive/weights/stage1_ca.h5')\n","\t\t\t\t\t\t\tself.embedding_compressor.save_weights('/content/drive/MyDrive/weights/stage1_embco.h5')\n","\t\t\t\t\t\t\tself.stage1_adversarial.save_weights('/content/drive/MyDrive/weights/stage1_adv.h5')\n","\n","\t\t\t\tself.stage1_generator.save_weights('/content/drive/MyDrive/weights/stage1_gen.h5')\n","\t\t\t\tself.stage1_discriminator.save_weights(\"/content/drive/MyDrive/weights/stage1_disc.h5\")\n"]},{"cell_type":"markdown","metadata":{"id":"inj5IwROxFgE"},"source":["## Check test folder for gernerated images from Stage1 Generator"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WUDlLFK0xFdV","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"4f18ee4d-de81-4bff-c3ce-d4d22d4a83c9","executionInfo":{"status":"error","timestamp":1702332135387,"user_tz":-330,"elapsed":447,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4ed86d02deb6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mStackGanStage1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_stage1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'StackGanStage1' is not defined"]}],"source":["StackGanStage1().train_stage1()"]},{"cell_type":"markdown","metadata":{"id":"4mjb95zqoKNk"},"source":["## Let's Implement Stage 2 Generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fIbdYNgoKNk"},"outputs":[],"source":["############################################################\n","# Stage 2 Generator Network\n","############################################################\n","\n","def concat_along_dims(inputs):\n","\t\"\"\"Joins the conditioned text with the encoded image along the dimensions.\n","\n","\tArgs:\n","\t\tinputs: consisting of conditioned text and encoded images as [c,x].\n","\n","\tReturns:\n","\t\tJoint block along the dimensions.\n","\t\"\"\"\n","\tc = inputs[0]\n","\tx = inputs[1]\n","\n","\tc = K.expand_dims(c, axis=1)\n","\tc = K.expand_dims(c, axis=1)\n","\tc = K.tile(c, [1, 16, 16, 1])\n","\treturn K.concatenate([c, x], axis = 3)\n","\n","def residual_block(input):\n","\t\"\"\"Residual block with plain identity connections.\n","\n","\tArgs:\n","\t\tinputs: input layer or an encoded layer\n","\n","\tReturns:\n","\t\tLayer with computed identity mapping.\n","\t\"\"\"\n","\tx = Conv2D(512, kernel_size=(3,3), padding='same', use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(input)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx = ReLU()(x)\n","\n","\tx = Conv2D(512, kernel_size=(3,3), padding='same', use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\n","\tx = add([x, input])\n","\tx = ReLU()(x)\n","\n","\treturn x\n","\n","def build_stage2_generator():\n","\t\"\"\"Build the Stage 2 Generator Network using the conditioning text and images from stage 1.\n","\n","\tReturns:\n","\t\tStage 2 Generator Model for StackGAN.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(1024,))\n","\tinput_images = Input(shape=(64, 64, 3))\n","\n","\t# Conditioning Augmentation\n","\tca = Dense(256)(input_layer1)\n","\tmls = LeakyReLU(alpha=0.2)(ca)\n","\tc = Lambda(conditioning_augmentation)(mls)\n","\n","\t# Downsampling block\n","\tx = ZeroPadding2D(padding=(1,1))(input_images)\n","\tx = Conv2D(128, kernel_size=(3,3), strides=1, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = ReLU()(x)\n","\n","\tx = ZeroPadding2D(padding=(1,1))(x)\n","\tx = Conv2D(256, kernel_size=(4,4), strides=2, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx = ReLU()(x)\n","\n","\tx = ZeroPadding2D(padding=(1,1))(x)\n","\tx = Conv2D(512, kernel_size=(4,4), strides=2, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx = ReLU()(x)\n","\n","\t# Concatenate text conditioning block with the encoded image\n","\tconcat = concat_along_dims([c, x])\n","\n","\t# Residual Blocks\n","\tx = ZeroPadding2D(padding=(1,1))(concat)\n","\tx = Conv2D(512, kernel_size=(3,3), use_bias=False, kernel_initializer='he_uniform')(x)\n","\tx = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x)\n","\tx = ReLU()(x)\n","\n","\tx = residual_block(x)\n","\tx = residual_block(x)\n","\tx = residual_block(x)\n","\tx = residual_block(x)\n","\n","\t# Upsampling Blocks\n","\tx = UpSamplingBlock(x, 512)\n","\tx = UpSamplingBlock(x, 256)\n","\tx = UpSamplingBlock(x, 128)\n","\tx = UpSamplingBlock(x, 64)\n","\n","\tx = Conv2D(3, kernel_size=(3,3), padding='same', use_bias=False, kernel_initializer='he_uniform')(x)\n","\tx = Activation('tanh')(x)\n","\n","\tstage2_gen = Model(inputs=[input_layer1, input_images], outputs=[x, mls])\n","\treturn stage2_gen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUuw447NoKNk","executionInfo":{"status":"ok","timestamp":1701882502067,"user_tz":-330,"elapsed":2275,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6cc71e46-07dd-4ce0-82b7-57606e9c7c8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_8\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_18 (InputLayer)       [(None, 64, 64, 3)]          0         []                            \n","                                                                                                  \n"," zero_padding2d (ZeroPaddin  (None, 66, 66, 3)            0         ['input_18[0][0]']            \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_20 (Conv2D)          (None, 64, 64, 128)          3456      ['zero_padding2d[0][0]']      \n","                                                                                                  \n"," re_lu_11 (ReLU)             (None, 64, 64, 128)          0         ['conv2d_20[0][0]']           \n","                                                                                                  \n"," zero_padding2d_1 (ZeroPadd  (None, 66, 66, 128)          0         ['re_lu_11[0][0]']            \n"," ing2D)                                                                                           \n","                                                                                                  \n"," input_17 (InputLayer)       [(None, 1024)]               0         []                            \n","                                                                                                  \n"," conv2d_21 (Conv2D)          (None, 32, 32, 256)          524288    ['zero_padding2d_1[0][0]']    \n","                                                                                                  \n"," dense_8 (Dense)             (None, 256)                  262400    ['input_17[0][0]']            \n","                                                                                                  \n"," batch_normalization_16 (Ba  (None, 32, 32, 256)          1024      ['conv2d_21[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_13 (LeakyReLU)  (None, 256)                  0         ['dense_8[0][0]']             \n","                                                                                                  \n"," re_lu_12 (ReLU)             (None, 32, 32, 256)          0         ['batch_normalization_16[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," lambda_3 (Lambda)           (None, 128)                  0         ['leaky_re_lu_13[0][0]']      \n","                                                                                                  \n"," zero_padding2d_2 (ZeroPadd  (None, 34, 34, 256)          0         ['re_lu_12[0][0]']            \n"," ing2D)                                                                                           \n","                                                                                                  \n"," tf.expand_dims (TFOpLambda  (None, 1, 128)               0         ['lambda_3[0][0]']            \n"," )                                                                                                \n","                                                                                                  \n"," conv2d_22 (Conv2D)          (None, 16, 16, 512)          2097152   ['zero_padding2d_2[0][0]']    \n","                                                                                                  \n"," tf.expand_dims_1 (TFOpLamb  (None, 1, 1, 128)            0         ['tf.expand_dims[0][0]']      \n"," da)                                                                                              \n","                                                                                                  \n"," batch_normalization_17 (Ba  (None, 16, 16, 512)          2048      ['conv2d_22[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," tf.tile (TFOpLambda)        (None, 16, 16, 128)          0         ['tf.expand_dims_1[0][0]']    \n","                                                                                                  \n"," re_lu_13 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_17[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," tf.concat (TFOpLambda)      (None, 16, 16, 640)          0         ['tf.tile[0][0]',             \n","                                                                     're_lu_13[0][0]']            \n","                                                                                                  \n"," zero_padding2d_3 (ZeroPadd  (None, 18, 18, 640)          0         ['tf.concat[0][0]']           \n"," ing2D)                                                                                           \n","                                                                                                  \n"," conv2d_23 (Conv2D)          (None, 16, 16, 512)          2949120   ['zero_padding2d_3[0][0]']    \n","                                                                                                  \n"," batch_normalization_18 (Ba  (None, 16, 16, 512)          2048      ['conv2d_23[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_14 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_18[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_24 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_14[0][0]']            \n","                                                                                                  \n"," batch_normalization_19 (Ba  (None, 16, 16, 512)          2048      ['conv2d_24[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_15 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_19[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_25 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_15[0][0]']            \n","                                                                                                  \n"," batch_normalization_20 (Ba  (None, 16, 16, 512)          2048      ['conv2d_25[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add (Add)                   (None, 16, 16, 512)          0         ['batch_normalization_20[0][0]\n","                                                                    ',                            \n","                                                                     're_lu_14[0][0]']            \n","                                                                                                  \n"," re_lu_16 (ReLU)             (None, 16, 16, 512)          0         ['add[0][0]']                 \n","                                                                                                  \n"," conv2d_26 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_16[0][0]']            \n","                                                                                                  \n"," batch_normalization_21 (Ba  (None, 16, 16, 512)          2048      ['conv2d_26[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_17 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_21[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_27 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_17[0][0]']            \n","                                                                                                  \n"," batch_normalization_22 (Ba  (None, 16, 16, 512)          2048      ['conv2d_27[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add_1 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_22[0][0]\n","                                                                    ',                            \n","                                                                     're_lu_16[0][0]']            \n","                                                                                                  \n"," re_lu_18 (ReLU)             (None, 16, 16, 512)          0         ['add_1[0][0]']               \n","                                                                                                  \n"," conv2d_28 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_18[0][0]']            \n","                                                                                                  \n"," batch_normalization_23 (Ba  (None, 16, 16, 512)          2048      ['conv2d_28[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_19 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_23[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_29 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_19[0][0]']            \n","                                                                                                  \n"," batch_normalization_24 (Ba  (None, 16, 16, 512)          2048      ['conv2d_29[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add_2 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_24[0][0]\n","                                                                    ',                            \n","                                                                     're_lu_18[0][0]']            \n","                                                                                                  \n"," re_lu_20 (ReLU)             (None, 16, 16, 512)          0         ['add_2[0][0]']               \n","                                                                                                  \n"," conv2d_30 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_20[0][0]']            \n","                                                                                                  \n"," batch_normalization_25 (Ba  (None, 16, 16, 512)          2048      ['conv2d_30[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_21 (ReLU)             (None, 16, 16, 512)          0         ['batch_normalization_25[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_31 (Conv2D)          (None, 16, 16, 512)          2359296   ['re_lu_21[0][0]']            \n","                                                                                                  \n"," batch_normalization_26 (Ba  (None, 16, 16, 512)          2048      ['conv2d_31[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add_3 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_26[0][0]\n","                                                                    ',                            \n","                                                                     're_lu_20[0][0]']            \n","                                                                                                  \n"," re_lu_22 (ReLU)             (None, 16, 16, 512)          0         ['add_3[0][0]']               \n","                                                                                                  \n"," up_sampling2d_8 (UpSamplin  (None, 32, 32, 512)          0         ['re_lu_22[0][0]']            \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_32 (Conv2D)          (None, 32, 32, 512)          2359296   ['up_sampling2d_8[0][0]']     \n","                                                                                                  \n"," batch_normalization_27 (Ba  (None, 32, 32, 512)          2048      ['conv2d_32[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_23 (ReLU)             (None, 32, 32, 512)          0         ['batch_normalization_27[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," up_sampling2d_9 (UpSamplin  (None, 64, 64, 512)          0         ['re_lu_23[0][0]']            \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_33 (Conv2D)          (None, 64, 64, 256)          1179648   ['up_sampling2d_9[0][0]']     \n","                                                                                                  \n"," batch_normalization_28 (Ba  (None, 64, 64, 256)          1024      ['conv2d_33[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_24 (ReLU)             (None, 64, 64, 256)          0         ['batch_normalization_28[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," up_sampling2d_10 (UpSampli  (None, 128, 128, 256)        0         ['re_lu_24[0][0]']            \n"," ng2D)                                                                                            \n","                                                                                                  \n"," conv2d_34 (Conv2D)          (None, 128, 128, 128)        294912    ['up_sampling2d_10[0][0]']    \n","                                                                                                  \n"," batch_normalization_29 (Ba  (None, 128, 128, 128)        512       ['conv2d_34[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_25 (ReLU)             (None, 128, 128, 128)        0         ['batch_normalization_29[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," up_sampling2d_11 (UpSampli  (None, 256, 256, 128)        0         ['re_lu_25[0][0]']            \n"," ng2D)                                                                                            \n","                                                                                                  \n"," conv2d_35 (Conv2D)          (None, 256, 256, 64)         73728     ['up_sampling2d_11[0][0]']    \n","                                                                                                  \n"," batch_normalization_30 (Ba  (None, 256, 256, 64)         256       ['conv2d_35[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," re_lu_26 (ReLU)             (None, 256, 256, 64)         0         ['batch_normalization_30[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_36 (Conv2D)          (None, 256, 256, 3)          1728      ['re_lu_26[0][0]']            \n","                                                                                                  \n"," activation_4 (Activation)   (None, 256, 256, 3)          0         ['conv2d_36[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 28645440 (109.27 MB)\n","Trainable params: 28632768 (109.23 MB)\n","Non-trainable params: 12672 (49.50 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["generator_stage2 = build_stage2_generator()\n","generator_stage2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjgQKX1DoKNl"},"outputs":[],"source":["\n","############################################################\n","# Stage 2 Discriminator Network\n","############################################################\n","\n","def build_stage2_discriminator():\n","\t\"\"\"Builds the Stage 2 Discriminator that uses the 256x256 resolution images from the generator\n","\tand the compressed and spatially replicated embeddings.\n","\n","\tReturns:\n","\t\tStage 2 Discriminator Model for StackGAN.\n","\t\"\"\"\n","\tinput_layer1 = Input(shape=(256, 256, 3))\n","\n","\tx = Conv2D(64, kernel_size=(4,4), padding='same', strides=2, use_bias=False,\n","\t\t\t\tkernel_initializer='he_uniform')(input_layer1)\n","\tx = LeakyReLU(alpha=0.2)(x)\n","\n","\tx = ConvBlock(x, 128)\n","\tx = ConvBlock(x, 256)\n","\tx = ConvBlock(x, 512)\n","\tx = ConvBlock(x, 1024)\n","\tx = ConvBlock(x, 2048)\n","\tx = ConvBlock(x, 1024, (1,1), 1)\n","\tx = ConvBlock(x, 512, (1,1), 1, False)\n","\n","\tx1 = ConvBlock(x, 128, (1,1), 1)\n","\tx1 = ConvBlock(x1, 128, (3,3), 1)\n","\tx1 = ConvBlock(x1, 512, (3,3), 1, False)\n","\n","\tx2 = add([x, x1])\n","\tx2 = LeakyReLU(alpha=0.2)(x2)\n","\n","\t# Concatenate compressed and spatially replicated embedding\n","\tinput_layer2 = Input(shape=(4, 4, 128))\n","\tconcat = concatenate([x2, input_layer2])\n","\n","\tx3 = Conv2D(512, kernel_size=(1,1), strides=1, padding='same', kernel_initializer='he_uniform')(concat)\n","\tx3 = BatchNormalization(gamma_initializer='ones', beta_initializer='zeros')(x3)\n","\tx3 = LeakyReLU(alpha=0.2)(x3)\n","\n","\t# Flatten and add a FC layer\n","\tx3 = Flatten()(x3)\n","\tx3 = Dense(1)(x3)\n","\tx3 = Activation('sigmoid')(x3)\n","\n","\tstage2_dis = Model(inputs=[input_layer1, input_layer2], outputs=[x3])\n","\treturn stage2_dis\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNPzE5cUoKNl","outputId":"015b914a-a82b-427d-e82b-692c81cddd0c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701882596372,"user_tz":-330,"elapsed":1728,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_13\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_27 (InputLayer)       [(None, 256, 256, 3)]        0         []                            \n","                                                                                                  \n"," conv2d_81 (Conv2D)          (None, 128, 128, 64)         3072      ['input_27[0][0]']            \n","                                                                                                  \n"," leaky_re_lu_18 (LeakyReLU)  (None, 128, 128, 64)         0         ['conv2d_81[0][0]']           \n","                                                                                                  \n"," conv2d_82 (Conv2D)          (None, 64, 64, 128)          131072    ['leaky_re_lu_18[0][0]']      \n","                                                                                                  \n"," batch_normalization_69 (Ba  (None, 64, 64, 128)          512       ['conv2d_82[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_19 (LeakyReLU)  (None, 64, 64, 128)          0         ['batch_normalization_69[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_83 (Conv2D)          (None, 32, 32, 256)          524288    ['leaky_re_lu_19[0][0]']      \n","                                                                                                  \n"," batch_normalization_70 (Ba  (None, 32, 32, 256)          1024      ['conv2d_83[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_20 (LeakyReLU)  (None, 32, 32, 256)          0         ['batch_normalization_70[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_84 (Conv2D)          (None, 16, 16, 512)          2097152   ['leaky_re_lu_20[0][0]']      \n","                                                                                                  \n"," batch_normalization_71 (Ba  (None, 16, 16, 512)          2048      ['conv2d_84[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_21 (LeakyReLU)  (None, 16, 16, 512)          0         ['batch_normalization_71[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_85 (Conv2D)          (None, 8, 8, 1024)           8388608   ['leaky_re_lu_21[0][0]']      \n","                                                                                                  \n"," batch_normalization_72 (Ba  (None, 8, 8, 1024)           4096      ['conv2d_85[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_22 (LeakyReLU)  (None, 8, 8, 1024)           0         ['batch_normalization_72[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_86 (Conv2D)          (None, 4, 4, 2048)           3355443   ['leaky_re_lu_22[0][0]']      \n","                                                          2                                       \n","                                                                                                  \n"," batch_normalization_73 (Ba  (None, 4, 4, 2048)           8192      ['conv2d_86[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_23 (LeakyReLU)  (None, 4, 4, 2048)           0         ['batch_normalization_73[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_87 (Conv2D)          (None, 4, 4, 1024)           2097152   ['leaky_re_lu_23[0][0]']      \n","                                                                                                  \n"," batch_normalization_74 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_87[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_24 (LeakyReLU)  (None, 4, 4, 1024)           0         ['batch_normalization_74[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_88 (Conv2D)          (None, 4, 4, 512)            524288    ['leaky_re_lu_24[0][0]']      \n","                                                                                                  \n"," batch_normalization_75 (Ba  (None, 4, 4, 512)            2048      ['conv2d_88[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," conv2d_89 (Conv2D)          (None, 4, 4, 128)            65536     ['batch_normalization_75[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," batch_normalization_76 (Ba  (None, 4, 4, 128)            512       ['conv2d_89[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_25 (LeakyReLU)  (None, 4, 4, 128)            0         ['batch_normalization_76[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_90 (Conv2D)          (None, 4, 4, 128)            147456    ['leaky_re_lu_25[0][0]']      \n","                                                                                                  \n"," batch_normalization_77 (Ba  (None, 4, 4, 128)            512       ['conv2d_90[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_26 (LeakyReLU)  (None, 4, 4, 128)            0         ['batch_normalization_77[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," conv2d_91 (Conv2D)          (None, 4, 4, 512)            589824    ['leaky_re_lu_26[0][0]']      \n","                                                                                                  \n"," batch_normalization_78 (Ba  (None, 4, 4, 512)            2048      ['conv2d_91[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," add_12 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_75[0][0]\n","                                                                    ',                            \n","                                                                     'batch_normalization_78[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," leaky_re_lu_27 (LeakyReLU)  (None, 4, 4, 512)            0         ['add_12[0][0]']              \n","                                                                                                  \n"," input_28 (InputLayer)       [(None, 4, 4, 128)]          0         []                            \n","                                                                                                  \n"," concatenate_6 (Concatenate  (None, 4, 4, 640)            0         ['leaky_re_lu_27[0][0]',      \n"," )                                                                   'input_28[0][0]']            \n","                                                                                                  \n"," conv2d_92 (Conv2D)          (None, 4, 4, 512)            328192    ['concatenate_6[0][0]']       \n","                                                                                                  \n"," batch_normalization_79 (Ba  (None, 4, 4, 512)            2048      ['conv2d_92[0][0]']           \n"," tchNormalization)                                                                                \n","                                                                                                  \n"," leaky_re_lu_28 (LeakyReLU)  (None, 4, 4, 512)            0         ['batch_normalization_79[0][0]\n","                                                                    ']                            \n","                                                                                                  \n"," flatten_2 (Flatten)         (None, 8192)                 0         ['leaky_re_lu_28[0][0]']      \n","                                                                                                  \n"," dense_15 (Dense)            (None, 1)                    8193      ['flatten_2[0][0]']           \n","                                                                                                  \n"," activation_9 (Activation)   (None, 1)                    0         ['dense_15[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 48486401 (184.96 MB)\n","Trainable params: 48472833 (184.91 MB)\n","Non-trainable params: 13568 (53.00 KB)\n","__________________________________________________________________________________________________\n"]}],"source":["discriminator_stage2 = build_stage2_discriminator()\n","discriminator_stage2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P9cc9zbPoKNl"},"outputs":[],"source":["\n","############################################################\n","# Stage 2 Adversarial Model\n","############################################################\n","\n","def stage2_adversarial_network(stage2_disc, stage2_gen, stage1_gen):\n","\t\"\"\"Stage 2 Adversarial Network.\n","\n","\tArgs:\n","\t\tstage2_disc: Stage 2 Discriminator Model.\n","\t\tstage2_gen: Stage 2 Generator Model.\n","\t\tstage1_gen: Stage 1 Generator Model.\n","\n","\tReturns:\n","\t\tStage 2 Adversarial network.\n","\t\"\"\"\n","\tconditioned_embedding = Input(shape=(1024, ))\n","\tlatent_space = Input(shape=(100, ))\n","\tcompressed_replicated = Input(shape=(4, 4, 128))\n","\n","\t#the discriminator is trained separately and stage1_gen already trained, and this is the reason why we freeze its layers by setting the property trainable=false\n","\tinput_images, ca = stage1_gen([conditioned_embedding, latent_space])\n","\tstage2_disc.trainable = False\n","\tstage1_gen.trainable = False\n","\n","\timages, ca2 = stage2_gen([conditioned_embedding, input_images])\n","\tprobability = stage2_disc([images, compressed_replicated])\n","\n","\treturn Model(inputs=[conditioned_embedding, latent_space, compressed_replicated],\n","\t\toutputs=[probability, ca2])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtdA-wgdoKNl","outputId":"2aa1ffd8-3c3b-42e6-cfb1-4c5015f7a319","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701882604911,"user_tz":-330,"elapsed":1706,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_14\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_29 (InputLayer)       [(None, 1024)]               0         []                            \n","                                                                                                  \n"," input_30 (InputLayer)       [(None, 100)]                0         []                            \n","                                                                                                  \n"," model (Functional)          [(None, 64, 64, 3),          1027040   ['input_29[0][0]',            \n","                              (None, 256)]                0          'input_30[0][0]']            \n","                                                                                                  \n"," model_8 (Functional)        [(None, 256, 256, 3),        2864544   ['input_29[0][0]',            \n","                              (None, 256)]                0          'model[1][0]']               \n","                                                                                                  \n"," input_31 (InputLayer)       [(None, 4, 4, 128)]          0         []                            \n","                                                                                                  \n"," model_13 (Functional)       (None, 1)                    4848640   ['model_8[0][0]',             \n","                                                          1          'input_31[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 87402241 (333.41 MB)\n","Trainable params: 28632768 (109.23 MB)\n","Non-trainable params: 58769473 (224.19 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["adversarial_stage2 = stage2_adversarial_network(discriminator_stage2, generator_stage2, generator)\n","adversarial_stage2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6uqTZoY6oKNm"},"outputs":[],"source":["class StackGanStage2(object):\n","\t\"\"\"StackGAN Stage 2 class.\n","\n","\tArgs:\n","\t\tepochs: Number of epochs\n","\t\tz_dim: Latent space dimensions\n","\t\tbatch_size: Batch Size\n","\t\tenable_function: If True, training function is decorated with tf.function\n","\t\tstage2_generator_lr: Learning rate for stage 2 generator\n","\t\tstage2_discriminator_lr: Learning rate for stage 2 discriminator\n","\t\"\"\"\n","\tdef __init__(self, epochs=500, z_dim=100, batch_size=64, enable_function=True, stage2_generator_lr=0.0002, stage2_discriminator_lr=0.0002):\n","\t\tself.epochs = epochs\n","\t\tself.z_dim = z_dim\n","\t\tself.enable_function = enable_function\n","\t\tself.stage1_generator_lr = stage2_generator_lr\n","\t\tself.stage1_discriminator_lr = stage2_discriminator_lr\n","\t\tself.low_image_size = 64\n","\t\tself.high_image_size = 256\n","\t\tself.conditioning_dim = 128\n","\t\tself.batch_size = batch_size\n","\t\tself.stage2_generator_optimizer = Adam(lr=stage2_generator_lr, beta_1=0.5, beta_2=0.999)\n","\t\tself.stage2_discriminator_optimizer = Adam(lr=stage2_discriminator_lr, beta_1=0.5, beta_2=0.999)\n","\t\tself.stage1_generator = build_stage1_generator()\n","\t\tself.stage1_generator.compile(loss='binary_crossentropy', optimizer=self.stage2_generator_optimizer)\n","\t\tself.stage1_generator.load_weights('/content/drive/MyDrive/weights/stage1_gen.h5')\n","\t\tself.stage2_generator = build_stage2_generator()\n","\t\tself.stage2_generator.compile(loss='binary_crossentropy', optimizer=self.stage2_generator_optimizer)\n","\n","\t\tself.stage2_discriminator = build_stage2_discriminator()\n","\t\tself.stage2_discriminator.compile(loss='binary_crossentropy', optimizer=self.stage2_discriminator_optimizer)\n","\n","\t\tself.ca_network = build_ca_network()\n","\t\tself.ca_network.compile(loss='binary_crossentropy', optimizer='Adam')\n","\n","\t\tself.embedding_compressor = build_embedding_compressor()\n","\t\tself.embedding_compressor.compile(loss='binary_crossentropy', optimizer='Adam')\n","\n","\t\tself.stage2_adversarial = stage2_adversarial_network(self.stage2_discriminator, self.stage2_generator, self.stage1_generator)\n","\t\tself.stage2_adversarial.compile(loss=['binary_crossentropy', adversarial_loss], loss_weights=[1, 2.0], optimizer=self.stage2_generator_optimizer)\n","\n","\t\tself.checkpoint2 = tf.train.Checkpoint(\n","        \tgenerator_optimizer=self.stage2_generator_optimizer,\n","        \tdiscriminator_optimizer=self.stage2_discriminator_optimizer,\n","        \tgenerator=self.stage2_generator,\n","        \tdiscriminator=self.stage2_discriminator,\n","        \tgenerator1=self.stage1_generator)\n","\n","\tdef visualize_stage2(self):\n","\t\t\"\"\"Running Tensorboard visualizations.\n","\t\t\"\"\"\n","\t\ttb = TensorBoard(log_dir=\"/content/drive/MyDrive/logs/\".format(time.time()))\n","\t\ttb.set_model(self.stage2_generator)\n","\t\ttb.set_model(self.stage2_discriminator)\n","\n","\tdef train_stage2(self):\n","\t\t\"\"\"Trains Stage 2 StackGAN.\n","\t\t\"\"\"\n","\t\tx_high_train, y_high_train, high_train_embeds = load_data(filename_path=\"/content/drive/MyDrive/birds/train/filenames.pickle\", class_id_path=\"/content/drive/MyDrive/birds/train/class_info.pickle\",dataset_path=\"/content/drive/MyDrive/CUB_200_2011\", embeddings_path=\"/content/drive/MyDrive/birds/train/char-CNN-RNN-embeddings.pickle\", size=(256, 256))\n","\t\tx_high_test, y_high_test, high_test_embeds = load_data(filename_path=\"/content/drive/MyDrive/birds/test/filenames.pickle\", class_id_path=\"/content/drive/MyDrive/birds/test/class_info.pickle\",dataset_path=\"/content/drive/MyDrive/CUB_200_2011\", embeddings_path=\"/content/drive/MyDrive/birds/test/char-CNN-RNN-embeddings.pickle\", size=(256, 256))\n","\t\tx_low_train, y_low_train, low_train_embeds = load_data(filename_path=\"/content/drive/MyDrive/birds/train/filenames.pickle\", class_id_path=\"/content/drive/MyDrive/birds/train/class_info.pickle\",dataset_path=\"/content/drive/MyDrive/CUB_200_2011\", embeddings_path=\"/content/drive/MyDrive/birds/train/char-CNN-RNN-embeddings.pickle\", size=(64, 64))\n","\t\tx_low_test, y_low_test, low_test_embeds = load_data(filename_path=\"/content/drive/MyDrive/birds/test/filenames.pickle\", class_id_path=\"/content/drive/MyDrive/birds/test/class_info.pickle\",dataset_path=\"/content/drive/MyDrive/CUB_200_2011\", embeddings_path=\"/content/drive/MyDrive/birds/test/char-CNN-RNN-embeddings.pickle\", size=(64, 64))\n","\n","\t\treal = np.ones((self.batch_size, 1), dtype='float') * 0.9\n","\t\tfake = np.zeros((self.batch_size, 1), dtype='float') * 0.1\n","\n","\t\tfor epoch in range(self.epochs):\n","\t\t\tprint(f'Epoch: {epoch}')\n","\n","\t\t\tgen_loss = []\n","\t\t\tdisc_loss = []\n","\n","\t\t\tnum_batches = int(x_high_train.shape[0] / self.batch_size)\n","\n","\t\t\tfor i in range(num_batches):\n","\n","\t\t\t\tlatent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n","\t\t\t\tembedding_text = high_train_embeds[i * self.batch_size:(i + 1) * self.batch_size]\n","\t\t\t\tcompressed_embedding = self.embedding_compressor.predict_on_batch(embedding_text)\n","\t\t\t\tcompressed_embedding = np.reshape(compressed_embedding, (-1, 1, 1, self.conditioning_dim))\n","\t\t\t\tcompressed_embedding = np.tile(compressed_embedding, (1, 4, 4, 1))\n","\n","\t\t\t\timage_batch = x_high_train[i * self.batch_size:(i+1) * self.batch_size]\n","\t\t\t\timage_batch = (image_batch - 127.5) / 127.5\n","\n","\t\t\t\tlow_res_fakes, _ = self.stage1_generator.predict([embedding_text, latent_space], verbose=3)\n","\t\t\t\thigh_res_fakes, _ = self.stage2_generator.predict([embedding_text, low_res_fakes], verbose=3)\n","\n","\t\t\t\tdiscriminator_loss = self.stage2_discriminator.train_on_batch([image_batch, compressed_embedding],\n","\t\t\t\t\tnp.reshape(real, (self.batch_size, 1)))\n","\n","\t\t\t\tdiscriminator_loss_gen = self.stage2_discriminator.train_on_batch([high_res_fakes, compressed_embedding],\n","\t\t\t\t\tnp.reshape(fake, (self.batch_size, 1)))\n","\n","\t\t\t\tdiscriminator_loss_fake = self.stage2_discriminator.train_on_batch([image_batch[:(self.batch_size-1)], compressed_embedding[1:]],\n","\t\t\t\t\tnp.reshape(fake[1:], (self.batch_size - 1, 1)))\n","\n","\t\t\t\td_loss = 0.5 * np.add(discriminator_loss, 0.5 * np.add(discriminator_loss_gen, discriminator_loss_fake))\n","\t\t\t\tdisc_loss.append(d_loss)\n","\n","\t\t\t\tprint(f'Discriminator Loss: {d_loss}')\n","\n","\t\t\t\tg_loss = self.stage2_adversarial.train_on_batch([embedding_text, latent_space, compressed_embedding],\n","\t\t\t\t\t[K.ones((self.batch_size, 1)) * 0.9, K.ones((self.batch_size, 256)) * 0.9])\n","\t\t\t\tgen_loss.append(g_loss)\n","\n","\t\t\t\tprint(f'Generator Loss: {g_loss}')\n","\n","\t\t\t\tif epoch % 5 == 0:\n","\t\t\t\t\tlatent_space = np.random.normal(0, 1, size=(self.batch_size, self.z_dim))\n","\t\t\t\t\tembedding_batch = high_test_embeds[0 : self.batch_size]\n","\n","\t\t\t\t\tlow_fake_images, _ = self.stage1_generator.predict([embedding_batch, latent_space], verbose=3)\n","\t\t\t\t\thigh_fake_images, _ = self.stage2_generator.predict([embedding_batch, low_fake_images], verbose=3)\n","\n","\t\t\t\t\tfor i, image in enumerate(high_fake_images[:10]):\n","\t\t\t\t\t    save_image(image, f'/content/drive/MyDrive/results_stage2/gen_{epoch}_{i}.png')\n","\n","\t\t\t\tif epoch % 10 == 0:\n","\t\t\t\t\tself.stage2_generator.save_weights('/content/drive/MyDrive/weights/stage2_gen.h5')\n","\t\t\t\t\tself.stage2_discriminator.save_weights(\"/content/drive/MyDrive/weights/stage2_disc.h5\")\n","\t\t\t\t\tself.ca_network.save_weights('/content/drive/MyDrive/weights/stage2_ca.h5')\n","\t\t\t\t\tself.embedding_compressor.save_weights('/content/drive/MyDrive/weights/stage2_embco.h5')\n","\t\t\t\t\tself.stage2_adversarial.save_weights('/content/drive/MyDrive/weights/stage2_adv.h5')\n","\n","\t\tself.stage2_generator.save_weights('/content/drive/MyDrive/weights/stage2_gen.h5')\n","\t\tself.stage2_discriminator.save_weights(\"/content/drive/MyDrive/weights/stage2_disc.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQILxsDGoKNm","outputId":"c041e363-5e63-4044-d727-f3215390dd72","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"error","timestamp":1702318547958,"user_tz":-330,"elapsed":3114,"user":{"displayName":"Aakash Mehta","userId":"12295696132520211875"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-f657e1ff0c89>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mStackGanStage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_stage2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-42256d288269>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, epochs, z_dim, batch_size, enable_function, stage2_generator_lr, stage2_discriminator_lr)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2_generator_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage1_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/weights/stage1_gen.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_stage2_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage2_generator_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'build_stage2_generator' is not defined"]}],"source":["StackGanStage2().train_stage2()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOZX1wBzoKNm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}